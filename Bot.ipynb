{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by [Gameli Ladzekpo](mailto:gameli.Ladzekpo@gmail.com) (Twitter/IG: @gamladz)\n",
    "\n",
    "For [AI Core](theaicore.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports \n",
    "\n",
    "import json \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from time import sleep, time\n",
    "import random\n",
    "import re\n",
    "import subprocess, os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_chrome function\n",
    "def open_chrome(port=9220, on_mac=True):\n",
    "    my_env = os.environ.copy()\n",
    "    if on_mac:\n",
    "        subprocess.Popen(['open', '-a', \"Google Chrome\", '--args', f'--remote-debugging-port={port}', 'http://www.example.com'], env=my_env)\n",
    "    else:\n",
    "        subprocess.Popen(f'google-chrome --remote-debugging-port={port} --user-data-dir=bots'.split(), env=my_env)\n",
    "\n",
    "class Bot():\n",
    "    def __init__(self, port_no = 9220, headless = False, verbose = False):\n",
    "        print('initialising bot')\n",
    "\n",
    "        open_chrome()\n",
    "\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\t# without this, the chrome webdriver can't start (SECURITY RISK)\n",
    "        options.add_experimental_option(f\"debuggerAddress\", f\"127.0.0.1:{port_no}\")\t# attach to the same port that you're running chrome on\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless\") # headless option allows scraper to run in the background\n",
    "        #options.add_argument(\"--window-size=1920x1080\")\n",
    "        self.driver = webdriver.Chrome('chrome_driver/chromedriver')\t\t\t# create webdriver\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def click_btn(self, text):\n",
    "        if self.verbose: print(f'clicking {text} btn')\n",
    "        element_types = ['button', 'div', 'input', 'a', 'label']\n",
    "        \n",
    "        for element_type in element_types:\n",
    "            btns = self.driver.fund_elements_by_xpath(f'//{element_type}')\n",
    "            # for btn in btns:\n",
    "            #   print(btn.text)\n",
    "\n",
    "            # SEARCH BY TEXT\n",
    "            try:\n",
    "                btn = [b for b in btns if b.text.lower() == text.lower()][0]\n",
    "                btn.click()\n",
    "                return\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            # SEARCH BY VALUE ATTRIBUTE IF NOT YET FOUND\n",
    "            try:\n",
    "                btn = self.driver.find_elements_by_xpath(f'//{element_type}[@value=\"{text}\"]')[0]\n",
    "                btn.click()\n",
    "                return\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        raise ValueError(f'button containing \"{text}\" not found')\n",
    "\n",
    "    def _search(self, query, _type='search', placeholder=None):\n",
    "        sleep(1)\n",
    "        s = self.driver.find_elements_by_xpath(f'//input[@type=\"{_type}\"]')\n",
    "        print(s)\n",
    "        if placeholder:\n",
    "            s = [i for i in s if i.get_attribute('placeholder').lower() == placeholder.lower()][0]\n",
    "        else:\n",
    "            s = s[0]\n",
    "        s.send_keys(query) \n",
    "\n",
    "    def toggle_verbose(self):\n",
    "        self.verbose = not self.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "initialising bot\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # EXAMPLE USAGE\n",
    "    bot = Bot()\n",
    "\n",
    "    searches = ['plant pots','plates', 'bin']\n",
    "    for search in searches:\n",
    "        bot.driver.get(f'https://www.ikea.com/gb/en/search/products/?q={search}')\n",
    "\n",
    "        # How to scroll on selenium\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                bot.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(5)\n",
    "                show_more_button = bot.driver.find_elements_by_xpath('//*[@class=\"show-more__button button button--secondary button--small\"]')[0]\n",
    "                show_more_button.click()\n",
    "            except:    \n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "        # Go into the main grid and find the link for each result\n",
    "        results = bot.driver.find_elements_by_xpath('//*[@id=\"search-results\"]/div/a')  \n",
    "        print (f'found {len(results)}) results for search \"{search}\" ')\n",
    "\n",
    "        pjpwj\n",
    "\n",
    "        results = [r.get_attribute('href') for r in results] \n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            bot.driver.get(result)\n",
    "            result = result.split('/')[-2]\n",
    "\n",
    "            # Product Description\n",
    "            prod_name = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-header-section__title--big\"]')\n",
    "            prod_name = [text.get_attribute('innerHTML') for text in prod_name]\n",
    "            prod_name = prod_name[0]\n",
    "            print(prod_name)\n",
    "\n",
    "            prod_price = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-price__integer\"]')\n",
    "            prod_price = [text.get_attribute('innerHTML') for text in prod_price]\n",
    "            prod_price = prod_price[0]\n",
    "            print(prod_price)\n",
    "\n",
    "            prod_desc = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-header-section__description-text\"]')\n",
    "            prod_desc = [text.get_attribute('innerHTML') for text in prod_desc]\n",
    "            prod_desc = prod_desc[0]\n",
    "            print(prod_desc)\n",
    "\n",
    "            joij\n",
    "\n",
    "\n",
    "\n",
    "            # First get the image - product dimensions image\n",
    "            prod_dims_images = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-product-dimensions-content__images\"]//img')\n",
    "            prod_dims_images = [i.get_attribute('src') for i in prod_dims_images]\n",
    " \n",
    "            for image in prod_dims_images:\n",
    "                print (prod_dims_images)\n",
    "\n",
    "                for idx, img_url in enumerate(prod_dims_images):\n",
    "                    filename = f'dims-{result}-{idx}'\n",
    "                    file_ext = img_url.split('.')[-1] \n",
    "                    file_ext = file_ext[ 0 : 3 ]\n",
    "                    urllib.request.urlretrieve(img_url, f'data/{result}/{filename}.{file_ext}')\n",
    "\n",
    "            # Second get the description text\n",
    "            prod_dims_name = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-product-dimensions__list-container\"]//dt')\n",
    "            prod_dims_name = [item.get_attribute('innerHTML').split(\":\")[0] for item in prod_dims_name]\n",
    "            \n",
    "            prod_dims_measure = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-product-dimensions__list-container\"]//dd')\n",
    "            prod_dims_measure = [item.get_attribute('innerHTML') for item in prod_dims_measure]\n",
    "            dims = dict(zip(prod_dims_name, prod_dims_measure))\n",
    "\n",
    "            print(dims)\n",
    "            \n",
    "\n",
    "            # Cycle through information on each page starting with images   \n",
    "            images = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-media-grid__media-container\"]//img') \n",
    "            images = [i.get_attribute('src') for i in images]\n",
    "\n",
    "            # Product Details\n",
    "            product_details = bot.driver.find_elements_by_xpath('//*[@class=\"range-revamp-product-details__paragraph\"]')\n",
    "            print (len(product_details))\n",
    "\n",
    "            # Materials\n",
    "            product_details = [paragraph.get_attribute('innerHTML') for paragraph in product_details]\n",
    "            product_details = \" \".join(product_details)\n",
    "\n",
    "            product_details_materials = bot.driver.find_elements_by_xpath('//*[@id=\"SEC_product-details-material-and-care\"]//span')\n",
    "            product_details_materials = [paragraph.get_attribute('innerHTML') for paragraph in product_details_materials]\n",
    "            product_details_materials = \",\".join(product_details_materials)\n",
    "\n",
    "\n",
    "            product_details_sustain = bot.driver.find_elements_by_xpath('//*[@id=\"SEC_product-details-sustainability-and-environment\"]//span')\n",
    "            product_details_sustain = [paragraph.get_attribute('innerHTML') for paragraph in product_details_sustain]\n",
    "            product_details_sustain = \",\".join(product_details_materials)\n",
    "             \n",
    "            # Packaging\n",
    "\n",
    "            os.makedirs(f'data/{result}', exist_ok=True)\n",
    "\n",
    "\n",
    "            # Get reviews\n",
    "            \n",
    "            for image in images:\n",
    "                # Loop through each image and save to disk\n",
    "\n",
    "                for idx, img_url in enumerate(images):\n",
    "\n",
    "                    # Create filename for each with ID and get the file extension\n",
    "                    filename = f'{result}-{idx}'\n",
    "                    file_ext = img_url.split('.')[-1] \n",
    "                    file_ext = file_ext[ 0 : 3 ]\n",
    "                    # Write to file\n",
    "                    urllib.request.urlretrieve(img_url, f'data/{result}/{filename}.{file_ext}')\n",
    "\n",
    "\n",
    " \n",
    "          \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}